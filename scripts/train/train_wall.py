#!/usr/bin/env python3
"""
Wall distance prediction training script.

Trains a 1D CNN model to predict left and right wall distances from lidar scans.
Data is loaded from NPZ files generated by waypoint_datagen.py.
"""

import argparse
import json
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

# Default configuration
DEFAULT_DATA_PATH = "data/datasets/lidar_tracking_Example_pure_pursuit_n4500.npz"
DEFAULT_OUTPUT_DIR = "data/models"
DEFAULT_EPOCHS = 300
DEFAULT_LEARNING_RATE = 5e-4
DEFAULT_WEIGHT_DECAY = 1e-4
DEFAULT_BATCH_SIZE = 64
DEFAULT_TEST_SPLIT = 0.2
DEFAULT_RANDOM_SEED = 42
DEFAULT_PATIENCE = 20
DEFAULT_INPUT_NOISE = 0.01
DEFAULT_LEFT_WEIGHT = 1.0
DEFAULT_RIGHT_WEIGHT = 1.5


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Train wall distance prediction model from lidar data."
    )

    parser.add_argument(
        "--data-path",
        type=str,
        default=DEFAULT_DATA_PATH,
        help=f"Path to NPZ data file. Default: {DEFAULT_DATA_PATH}",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default=DEFAULT_OUTPUT_DIR,
        help=f"Directory to save trained models. Default: {DEFAULT_OUTPUT_DIR}",
    )

    parser.add_argument(
        "--epochs",
        type=int,
        default=DEFAULT_EPOCHS,
        help=f"Number of training epochs. Default: {DEFAULT_EPOCHS}",
    )

    parser.add_argument(
        "--learning-rate",
        type=float,
        default=DEFAULT_LEARNING_RATE,
        help=f"Learning rate. Default: {DEFAULT_LEARNING_RATE}",
    )

    parser.add_argument(
        "--weight-decay",
        type=float,
        default=DEFAULT_WEIGHT_DECAY,
        help=f"Weight decay (L2 regularization). Default: {DEFAULT_WEIGHT_DECAY}",
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"Batch size for training. Default: {DEFAULT_BATCH_SIZE}",
    )

    parser.add_argument(
        "--test-split",
        type=float,
        default=DEFAULT_TEST_SPLIT,
        help=f"Test set split ratio. Default: {DEFAULT_TEST_SPLIT}",
    )

    parser.add_argument(
        "--random-seed",
        type=int,
        default=DEFAULT_RANDOM_SEED,
        help=f"Random seed for reproducibility. Default: {DEFAULT_RANDOM_SEED}",
    )

    parser.add_argument(
        "--patience",
        type=int,
        default=DEFAULT_PATIENCE,
        help=f"Early stopping patience. Default: {DEFAULT_PATIENCE}",
    )

    parser.add_argument(
        "--input-noise",
        type=float,
        default=DEFAULT_INPUT_NOISE,
        help=f"Input noise std for training augmentation. Default: {DEFAULT_INPUT_NOISE}",
    )

    parser.add_argument(
        "--left-weight",
        type=float,
        default=DEFAULT_LEFT_WEIGHT,
        help=f"Loss weight for left wall distance. Default: {DEFAULT_LEFT_WEIGHT}",
    )

    parser.add_argument(
        "--right-weight",
        type=float,
        default=DEFAULT_RIGHT_WEIGHT,
        help=f"Loss weight for right wall distance. Default: {DEFAULT_RIGHT_WEIGHT}",
    )

    return parser.parse_args()


class Lidar1DCNN(nn.Module):
    """1D CNN for wall distance prediction from lidar scans."""

    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv1d(2, 32, kernel_size=7, padding=3),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
        )
        self.regressor = nn.Sequential(
            nn.Flatten(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 2)
        )

    def forward(self, x):
        x = self.features(x)
        return self.regressor(x)


def add_angle_channel(X, angles):
    """Add angle channel to lidar data.

    Args:
        X: Lidar data of shape (N, 1080)
        angles: Angle values for each lidar point

    Returns:
        Data with angle channel of shape (N, 2, 1080)
    """
    ang = np.tile(angles, (X.shape[0], 1))
    return np.stack([X, ang], axis=1)


def generate_model_filename(args, prefix="wall"):
    """Generate parameterized model filename.

    Args:
        args: Parsed arguments
        prefix: Filename prefix

    Returns:
        Filename string
    """
    lr_str = f"{args.learning_rate:.0e}".replace("e-0", "e-")
    return f"{prefix}_model_e{args.epochs}_lr{lr_str}_bs{args.batch_size}.pth"


def generate_metrics_filename(args, prefix="wall"):
    """Generate parameterized metrics filename.

    Args:
        args: Parsed arguments
        prefix: Filename prefix

    Returns:
        Filename string
    """
    lr_str = f"{args.learning_rate:.0e}".replace("e-0", "e-")
    return f"{prefix}_mse_e{args.epochs}_lr{lr_str}_bs{args.batch_size}.json"


def main():
    """Main execution function."""
    args = parse_args()

    # Set random seeds
    torch.manual_seed(args.random_seed)
    np.random.seed(args.random_seed)

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("WALL DISTANCE PREDICTION TRAINING")
    print("=" * 60)

    # -----------------------------
    # 1. Load dataset
    # -----------------------------
    print(f"\nLoading data from {args.data_path}...")
    data = np.load(args.data_path)
    X = data["scans"]  # Changed from "lidar" to match waypoint_datagen output
    y_left = data["left_wall_dist"]
    y_right = data["right_wall_dist"]

    print(f"Dataset loaded: {X.shape}")

    # -----------------------------
    # 2. Targets: log-distance
    # -----------------------------
    y = np.stack([y_left, y_right], axis=1)
    y = np.log(y + 1e-3)  # log-distance target

    # -----------------------------
    # 3. Train / test split
    # -----------------------------
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=args.test_split, random_state=args.random_seed
    )

    # -----------------------------
    # 4. Global normalization (train-set only)
    # -----------------------------
    X_mean = X_train.mean()
    X_std = X_train.std() + 1e-6

    X_train = (X_train - X_mean) / X_std
    X_test = (X_test - X_mean) / X_std

    # -----------------------------
    # 5. Angle channel
    # -----------------------------
    angles = np.linspace(-1.0, 1.0, X.shape[1])
    X_train = add_angle_channel(X_train, angles)
    X_test = add_angle_channel(X_test, angles)

    # -----------------------------
    # 6. Optional input noise (train only)
    # -----------------------------
    if args.input_noise > 0:
        X_train += np.random.normal(0, args.input_noise, X_train.shape)

    # -----------------------------
    # 7. Torch datasets
    # -----------------------------
    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    y_train_t = torch.tensor(y_train, dtype=torch.float32)
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    y_test_t = torch.tensor(y_test, dtype=torch.float32)

    train_ds = TensorDataset(X_train_t, y_train_t)
    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)

    print(f"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

    # -----------------------------
    # 8. Model
    # -----------------------------
    model = Lidar1DCNN()
    print(f"\nModel architecture:\n{model}")

    # -----------------------------
    # 9. Loss (asymmetric weighting)
    # -----------------------------
    base_loss = nn.SmoothL1Loss(reduction="none")
    loss_weights = torch.tensor([args.left_weight, args.right_weight])

    def weighted_loss(pred, target):
        loss = base_loss(pred, target)
        loss = loss * loss_weights.to(loss.device)
        return loss.mean()

    optimizer = optim.Adam(
        model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay
    )

    # -----------------------------
    # 10. Training loop with early stopping
    # -----------------------------
    print(f"\nTraining for up to {args.epochs} epochs...")
    print(f"Loss weights: left={args.left_weight}, right={args.right_weight}")
    print("=" * 60)

    best_loss = float("inf")
    patience_ctr = 0

    for epoch in range(args.epochs):
        model.train()
        train_loss = 0.0

        for xb, yb in train_loader:
            optimizer.zero_grad()
            pred = model(xb)
            loss = weighted_loss(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        train_loss /= len(train_loader)

        model.eval()
        with torch.no_grad():
            test_pred = model(X_test_t)
            test_loss = weighted_loss(test_pred, y_test_t).item()

        print(
            f"Epoch {epoch + 1:03d} | Train: {train_loss:.4f} | Test: {test_loss:.4f}"
        )

        if test_loss < best_loss:
            best_loss = test_loss
            patience_ctr = 0
            # Save best model
            model_filename = generate_model_filename(args)
            model_path = output_dir / model_filename
            torch.save(model.state_dict(), model_path)
        else:
            patience_ctr += 1
            if patience_ctr >= args.patience:
                print(f"\nEarly stopping at epoch {epoch + 1}")
                break

    # -----------------------------
    # 11. Save metrics
    # -----------------------------
    metrics = {
        "final_test_loss": float(test_loss),
        "best_test_loss": float(best_loss),
        "epochs_trained": epoch + 1,
        "parameters": {
            "learning_rate": args.learning_rate,
            "batch_size": args.batch_size,
            "weight_decay": args.weight_decay,
            "epochs": args.epochs,
            "test_split": args.test_split,
            "left_weight": args.left_weight,
            "right_weight": args.right_weight,
        },
    }

    metrics_filename = generate_metrics_filename(args)
    metrics_path = output_dir / metrics_filename
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=2)

    print("\n" + "=" * 60)
    print(f"Best test loss: {best_loss:.4f}")
    print(f"Model saved to: {model_path}")
    print(f"Metrics saved to: {metrics_path}")

    # -----------------------------
    # 12. Example inference (convert back to meters)
    # -----------------------------
    model.load_state_dict(torch.load(model_path))
    model.eval()

    with torch.no_grad():
        pred_log = model(X_test_t[:1]).squeeze().numpy()
        pred = np.exp(pred_log)

        true = np.exp(y_test[:1].squeeze())

        print("\nExample prediction:")
        print(f"  Predicted distances: left={pred[0]:.3f} m, right={pred[1]:.3f} m")
        print(f"  True distances:      left={true[0]:.3f} m, right={true[1]:.3f} m")

    print("\nTraining complete!")
    print("=" * 60)


if __name__ == "__main__":
    main()
