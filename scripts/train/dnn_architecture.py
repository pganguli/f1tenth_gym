#!/usr/bin/env python3
"""
DNN architecture training script for F1TENTH lidar-based prediction.

This script trains multiple CNN architectures to predict:
- Model 1: Left wall distance
- Model 2: Right wall distance
- Model 3: Heading error

Data is loaded from NPZ files generated by waypoint_datagen.py.
"""

import argparse
import json
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

# Default configuration
DEFAULT_DATA_PATH = "data/datasets/lidar_tracking_Example_pure_pursuit_n4500.npz"
DEFAULT_OUTPUT_DIR = "data/models"
DEFAULT_ARCHITECTURE = 3
DEFAULT_EPOCHS = 13
DEFAULT_TRIALS = 3
DEFAULT_BATCH_SIZE = 32
DEFAULT_TEST_SPLIT = 0.2
DEFAULT_RANDOM_SEED = 42
DEFAULT_LEARNING_RATE = 1e-3
DEFAULT_WEIGHT_DECAY = 1e-4


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Train DNN architectures for F1TENTH lidar-based prediction."
    )

    parser.add_argument(
        "--data-path",
        type=str,
        default=DEFAULT_DATA_PATH,
        help=f"Path to NPZ data file. Default: {DEFAULT_DATA_PATH}",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default=DEFAULT_OUTPUT_DIR,
        help=f"Directory to save trained models. Default: {DEFAULT_OUTPUT_DIR}",
    )

    parser.add_argument(
        "--architecture",
        type=int,
        choices=[1, 2, 3, 4, 5, 6, 7],
        default=DEFAULT_ARCHITECTURE,
        help=f"Architecture to use (1-7, increasing complexity). Default: {DEFAULT_ARCHITECTURE}",
    )

    parser.add_argument(
        "--epochs",
        type=int,
        default=DEFAULT_EPOCHS,
        help=f"Number of training epochs. Default: {DEFAULT_EPOCHS}",
    )

    parser.add_argument(
        "--trials",
        type=int,
        default=DEFAULT_TRIALS,
        help=f"Number of training trials per model. Default: {DEFAULT_TRIALS}",
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"Batch size for training. Default: {DEFAULT_BATCH_SIZE}",
    )

    parser.add_argument(
        "--test-split",
        type=float,
        default=DEFAULT_TEST_SPLIT,
        help=f"Test set split ratio. Default: {DEFAULT_TEST_SPLIT}",
    )

    parser.add_argument(
        "--random-seed",
        type=int,
        default=DEFAULT_RANDOM_SEED,
        help=f"Random seed for reproducibility. Default: {DEFAULT_RANDOM_SEED}",
    )

    parser.add_argument(
        "--learning-rate",
        type=float,
        default=DEFAULT_LEARNING_RATE,
        help=f"Learning rate. Default: {DEFAULT_LEARNING_RATE}",
    )

    parser.add_argument(
        "--weight-decay",
        type=float,
        default=DEFAULT_WEIGHT_DECAY,
        help=f"Weight decay (L2 regularization). Default: {DEFAULT_WEIGHT_DECAY}",
    )

    return parser.parse_args()


class LidarCNN(nn.Module):
    """Base class for CNN architectures processing lidar data."""

    def __init__(self, arch_num: int):
        """Initialize the architecture.

        Args:
            arch_num: Architecture number (1-7)
        """
        super().__init__()
        self.arch_num = arch_num
        self.features = self._build_features()

    def _build_features(self) -> nn.Sequential:
        """Build the feature extraction layers based on architecture number."""
        raise NotImplementedError

    def forward(self, x):
        """Forward pass.

        Args:
            x: Input tensor of shape (batch_size, 1080)

        Returns:
            Output tensor of shape (batch_size, 1)
        """
        # Reshape to (batch_size, 1, 1080) for Conv1d
        x = x.unsqueeze(1)
        x = self.features(x)
        return x


class Architecture1(LidarCNN):
    """Minimal architecture (~24 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 1, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=8),
            nn.Flatten(),
            nn.Linear(134, 1),  # (1080 - 3 + 1) / 8 = 1078 / 8 = 134
        )


class Architecture2(LidarCNN):
    """Small architecture (~66 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 1, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Conv1d(1, 1, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(1, 1, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Flatten(),
            nn.Linear(32, 8),  # (((1078/4-2)/2)-2)/4 = 32
            nn.ELU(),
            nn.Linear(8, 1),
        )


class Architecture3(LidarCNN):
    """Medium architecture (~192 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 1, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(1, 8, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Flatten(),
            nn.Linear(
                1072, 32
            ),  # (1080-2)/2 = 539, (539-2)/4 = 134, * 8 channels = 1072
            nn.ELU(),
            nn.Linear(32, 1),
        )


class Architecture4(LidarCNN):
    """Medium-large architecture (~326 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 1, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(1, 16, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Flatten(),
            nn.Linear(
                2144, 32
            ),  # (1080-2)/2 = 539, (539-2)/4 = 134, * 16 channels = 2144
            nn.ELU(),
            nn.Linear(32, 1),
        )


class Architecture5(LidarCNN):
    """Large architecture (~596 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 8, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(8, 16, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Flatten(),
            nn.Linear(
                2144, 64
            ),  # (1080-2)/2 = 539, (539-2)/4 = 134, * 16 channels = 2144
            nn.ELU(),
            nn.Linear(64, 1),
        )


class Architecture6(LidarCNN):
    """Very large architecture (~1113 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 8, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(8, 32, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Flatten(),
            nn.Linear(
                4288, 64
            ),  # (1080-2)/2 = 539, (539-2)/4 = 134, * 32 channels = 4288
            nn.ELU(),
            nn.Linear(64, 1),
        )


class Architecture7(LidarCNN):
    """Largest architecture (~2194 KB)."""

    def _build_features(self):
        return nn.Sequential(
            nn.Conv1d(1, 16, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(16, 32, kernel_size=3, padding=0),
            nn.ELU(),
            nn.MaxPool1d(kernel_size=4),
            nn.Flatten(),
            nn.Linear(
                4288, 128
            ),  # (1080-2)/2 = 539, (539-2)/4 = 134, * 32 channels = 4288
            nn.ELU(),
            nn.Linear(128, 1),
        )


def create_architecture(arch_num: int) -> nn.Module:
    """Create a CNN architecture for lidar data processing.

    Args:
        arch_num: Architecture number (1-7), higher numbers = more complex

    Returns:
        PyTorch model with the specified architecture

    Architecture sizes (approximate):
        1: ~24 KB
        2: ~66 KB
        3: ~192 KB
        4: ~326 KB
        5: ~596 KB
        6: ~1113 KB
        7: ~2194 KB
    """
    architectures = {
        1: Architecture1,
        2: Architecture2,
        3: Architecture3,
        4: Architecture4,
        5: Architecture5,
        6: Architecture6,
        7: Architecture7,
    }

    if arch_num not in architectures:
        raise ValueError(f"Architecture number must be 1-7, got {arch_num}")

    return architectures[arch_num](arch_num)


def load_and_preprocess_data(
    data_path: str, test_split: float, random_seed: int
) -> Dict[str, np.ndarray]:
    """Load data from NPZ file and prepare train/test splits.

    Args:
        data_path: Path to NPZ data file
        test_split: Fraction of data to use for testing
        random_seed: Random seed for reproducibility

    Returns:
        Dictionary containing train/test splits for X and all targets
    """
    print(f"Loading data from {data_path}...")
    data = np.load(data_path)

    # Extract data
    X = data["scans"]  # Shape: (n_samples, 1080)
    y_left = data["left_wall_dist"]  # Shape: (n_samples,)
    y_right = data["right_wall_dist"]  # Shape: (n_samples,)
    y_heading = data["heading_error"]  # Shape: (n_samples,)

    print(f"Loaded {X.shape[0]} samples with {X.shape[1]} lidar points each")

    # Split data
    X_train, X_test, y_left_train, y_left_test = train_test_split(
        X, y_left, test_size=test_split, random_state=random_seed
    )
    _, _, y_right_train, y_right_test = train_test_split(
        X, y_right, test_size=test_split, random_state=random_seed
    )
    _, _, y_heading_train, y_heading_test = train_test_split(
        X, y_heading, test_size=test_split, random_state=random_seed
    )

    print(f"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

    return {
        "X_train": X_train,
        "X_test": X_test,
        "y_left_train": y_left_train,
        "y_left_test": y_left_test,
        "y_right_train": y_right_train,
        "y_right_test": y_right_test,
        "y_heading_train": y_heading_train,
        "y_heading_test": y_heading_test,
    }


def train_model(
    model: nn.Module,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    epochs: int,
    batch_size: int,
    learning_rate: float,
    weight_decay: float,
) -> Tuple[float, float]:
    """Train a single model.

    Args:
        model: PyTorch model to train
        X_train: Training features
        y_train: Training targets
        X_test: Test features
        y_test: Test targets
        epochs: Number of training epochs
        batch_size: Batch size
        learning_rate: Learning rate
        weight_decay: Weight decay (L2 regularization)

    Returns:
        Tuple of (test_loss, test_mse)
    """
    # Convert to torch tensors
    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

    # Create data loader
    train_dataset = TensorDataset(X_train_t, y_train_t)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Setup optimizer and loss
    optimizer = optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=weight_decay
    )
    criterion = nn.MSELoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0.0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

    # Evaluation
    model.eval()
    with torch.no_grad():
        test_pred = model(X_test_t)
        test_loss = criterion(test_pred, y_test_t).item()
        test_mse = test_loss  # MSE is the same as loss for MSELoss

    return test_loss, test_mse


def main():
    """Main execution function."""
    args = parse_args()

    # Set random seeds for reproducibility
    torch.manual_seed(args.random_seed)
    np.random.seed(args.random_seed)

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load and preprocess data
    data = load_and_preprocess_data(args.data_path, args.test_split, args.random_seed)

    # Storage for MSE values
    store_mse = {
        f"model_left_arch{args.architecture}": 0.0,
        f"model_right_arch{args.architecture}": 0.0,
        f"model_heading_arch{args.architecture}": 0.0,
    }

    print(f"\nTraining architecture {args.architecture} for {args.trials} trials...")
    print("=" * 60)

    # Train models for each target
    models_to_train = [
        ("left", data["y_left_train"], data["y_left_test"]),
        ("right", data["y_right_train"], data["y_right_test"]),
        ("heading", data["y_heading_train"], data["y_heading_test"]),
    ]

    for model_name, y_train, y_test in models_to_train:
        print(f"\n{model_name.upper()} DISTANCE MODEL")
        print("-" * 60)

        for trial in range(args.trials):
            print(f"Trial {trial + 1}/{args.trials}...", end=" ")

            # Create and train model
            model = create_architecture(args.architecture)
            loss, mse = train_model(
                model,
                data["X_train"],
                y_train,
                data["X_test"],
                y_test,
                args.epochs,
                args.batch_size,
                args.learning_rate,
                args.weight_decay,
            )

            # Accumulate MSE
            store_mse[f"model_{model_name}_arch{args.architecture}"] += mse
            print(f"MSE: {mse:.6f}")

            # Save model (only from last trial)
            if trial == args.trials - 1:
                model_path = (
                    output_dir / f"lidar_{model_name}_arch{args.architecture}.pth"
                )
                torch.save(model.state_dict(), model_path)

                # Get file size
                file_size = model_path.stat().st_size / 1024  # KB
                print(f"Saved to {model_path} ({file_size:.2f} KB)")

    # Calculate average MSE
    print("\n" + "=" * 60)
    print("AVERAGE MSE ACROSS TRIALS")
    print("=" * 60)

    average_mse = {}
    for key, total_mse in store_mse.items():
        avg = total_mse / args.trials
        average_mse[key] = float(avg)
        print(f"{key}: {avg:.6f}")

    # Save metrics to JSON
    metrics_path = output_dir / f"mse_arch{args.architecture}.json"
    with open(metrics_path, "w") as f:
        json.dump(average_mse, f, indent=2)

    print(f"\nMetrics saved to {metrics_path}")
    print("Training complete!")


if __name__ == "__main__":
    main()
